- need to find out how large the cluster is
    - if its too small for all of this data, how do we navigate this problem of data storage?
    - where specifically is the data stored? manager node or worker nodes? if the data is too large, is it always stored totally on worker nodes?
    - when calling via API end point, do we chunk pull the data? 
    - 
- do our worker nodes need to be individually updated with python3? 
    - if we manually do this, can we write about how to programmatically do this? 
    - each worker is doing the work....so if we ask it to run python 3 and it only has python 2, i imagine the work won't get done correctly


The choice it hardcode

How we uplaod the data

The default dict csv

The use of fgm_sum threshold

Last case condition

talk about why fgm going to 2 is both low, but also most probable for statistically significance (ignoring just 1, but weighing in those who score twice in a game and primarily on the same exact defender)...5 attempts is more of a weighting factor. 