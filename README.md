# Big Data Computing Project 1

Authors: Ali Nazim, Chris Kuzemka

# Breakdown

Housed within this repository are MapReduce programs corresponding to two unique sub-projects. Each sub-project relies on the use of a free dataset to answer specific questions provided by our professor. 

One dataset features **NYC Parking Violations** and is provided by the NYC Department of Finance. This dataset showcases public information on parking tickets provided to specific vehicles and includes vehicular features such as plate numbers, registration, car model, car color, etc. It also will feature the time and location of such tickets and other more exogenous variables. 

Another dataset features **NBA Shot Logs** collected across the 2014-2015 season. This dataset focuses primarily on how a basketball player shoots in a game including if the shot was successful or not, who was the closest dedfender, match dates and time instance metrics, and more. 

The primary repository code resides in the `src` folder which will house two distinct folders corresponding to each project: `nba` and `parking`. Each folder was worked on by an individual developer and their MapReduce programs to answer specific questions for these sub-projects are all collected in here. 

This respository is best git cloned into the Google VM cluster environment provided in the Big Data Computing graduate course at Fordham University. Within such clusters, after some light cluster configuration for the project, one should be able to run individual MapReduce shell scripts that will activate sequential ordering of Python files to stream data and achieve desired outputs. 

It's important to last note that our `.gitignore` file calls a `data` directory at the root hierarchy of this repository. This `data` directory can be generated by running an individual data pull shell script described within the sub-project READMEs. No data should ever be uploaded to this repository. 


# Project Setup


Here are a list of steps to best guide one student in this Fordham class to run this repository. 

## Dependancies 
1) Ensure your cluster has Python 3 installed. This project strictly relies on Python 3 for all of the mapper and reducer files. 
1) Ensure bash scripts are runnable on your cluster. 
1) Ensure git is active on your cluster. 
1) Ensure the Python package `csv` is installed on your cluster.
1) Ensure `kaggle` is setup properly on your cluster as well as other dependencies reliant on API usage. Details of such provided within sub-project READMEs and corresponding shell scripts housing PATHs. 

## Location
1) Sign into your Fordham Google VM cluster via external `ssh`
1) Switch user to your `root` directory (`su root`) and enter a preset password if prompted. 
1) Change directory to the elevated root directory (`cd /`).
1) Change directory to the `mapreduce-test` directory (`cd mapreduce-test`).
1) Make a new directory called `mapreduce-project1` (`mkdir mapreduce-project1`).
1) Within such directory, `git clone` this repository either via `ssh` or `https`. For our case, we leveraged `https` which allowed a more seamless cloning. `ls` the available directories within this current directory. You should see a new directory called `big_data_project_1`
1) Enter this `big_data_project_1` (`cd big_data_project_1`) and run a `git status` and `git fetch` to verify you have successful installation of the project and are connected upstream to origin. 

## Final Notes
1) Change directory to the `src` folder (`cd src`) and freely choose between both folders to run a MapReduce program. Be sure to follow any prompted README instructions within such folder for appropriate project run. 